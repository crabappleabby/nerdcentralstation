{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crabappleabby/nerdcentralstation/blob/main/Notebook1__ForestFire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 1: Introduction to Machine Learning\n",
        "\n",
        "Please make a copy of this notebook and share it with us once you have completed it. Our email addresses are: laura_lxm_ma@berkeley.edu\n",
        "and aditi.telang@berkeley.edu\n",
        "\n"
      ],
      "metadata": {
        "id": "go1wfySW1uWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Machine Learning Model"
      ],
      "metadata": {
        "id": "w5jXq_t_5KZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will serve as an introduction to machine learning. Please read  [this](https://ischoolonline.berkeley.edu/blog/what-is-machine-learning/) article that defines machine learning (please read up to the deep learning section)."
      ],
      "metadata": {
        "id": "ZQDYtyj32YpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:**\n",
        "\n",
        " Please edit this cell and state two use cases for machine learning below:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oqw0rMK32xqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Machine Learning Model"
      ],
      "metadata": {
        "id": "qnVu8h3i5FJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our project will we be working with a supervised machine learning model. This means that the dataset we will be working with has been pre-labeled and classified by the user. For example, if we are trying to build a machine learning model that can classify text as either \"negative\" or \"positive\", a pre-classified dataset for a machine learning model may look something like this:\n",
        "\n",
        "\n",
        "|   Text                                             | Sentiment |\n",
        "|---------------------------------------------------|-----------|\n",
        "| I loved the movie! It was fantastic.              | Positive  |\n",
        "| This restaurant has terrible service.              | Negative  |\n",
        "| The weather is beautiful today.                   | Positive  |\n",
        "| The product arrived broken and unusable.          | Negative  |\n",
        "| The new update improved the app's performance.    | Positive  |\n",
        "| I'm really disappointed with their customer support.| Negative |\n",
        "| What a great experience at the amusement park!    | Positive  |\n",
        "| The quality of this product exceeded my expectations.| Positive |\n",
        "| This book is poorly written and hard to follow.    | Negative  |\n",
        "| The hiking trail was so disappointing.            | Negative  |\n",
        "\n",
        "\n",
        "\n",
        "In this example, the \"Text\" column contains the input text data, and the \"Sentiment\" column contains the corresponding labels indicating whether the sentiment of the text is positive or negative. This data can be used to train a supervised machine learning model to predict sentiment labels for new, unseen text inputs."
      ],
      "metadata": {
        "id": "bHGJSzX_4KZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps involved in supervised learning include:\n",
        "\n",
        "**1. Data Collection and Preparation:**\n",
        "\n",
        "Gather a dataset that consists of input data (features) and their corresponding labels (target values). The dataset should be diverse and representative of the real-world scenarios you're addressing. Preprocess the data to clean, transform, and format it for use in training.\n",
        "\n",
        "**2. Data Splitting:**\n",
        "\n",
        "Divide the dataset into two subsets: a training set and a test/validation set. The training set will be used to train the model, while the test/validation set is used to evaluate the model's performance on new, unseen data. This helps assess how well the model generalizes.\n",
        "\n",
        "**3. Feature Engineering or Extraction:**\n",
        "\n",
        "Convert the raw input data into numerical features that the machine learning algorithm can work with. Depending on the nature of the data, this could involve techniques like normalization, standardization, encoding categorical variables, and creating new features. We will worry less about this step within our project! But, it is important to know that different machine learning models have different requirements and assumptions about the format of the input data. For example, many machine learning algorithms cannot directly handle categorical features (e.g., \"red,\" \"green,\" \"blue\"). These features need to be turned into (encoded) into numerical values through techniques like one-hot encoding or label encoding, so the model can process them effectively.\n",
        "\n",
        "**4.  Model Selection:**\n",
        "\n",
        "Choose an appropriate machine learning algorithm or model architecture that suits your problem. The choice of algorithm depends on the type of task (classification, regression, etc.) and the characteristics of the data. We will explore this later on!\n",
        "\n",
        "**5. Model Training:**\n",
        "\n",
        "Feed the training data (feature representations of input data and their corresponding labels) into the chosen model. The model learns the underlying patterns and relationships between the features and labels during this phase. Training involves adjusting model parameters to minimize the difference between predicted and actual labels.\n",
        "\n",
        "**6. Model Evaluation:**\n",
        "\n",
        "After training, assess the model's performance on the test/validation set that it hasn't seen before. Use evaluation metrics to quantify how well the model is performing. Common evaluation metrics include accuracy, precision, recall, F1-score, and Mean Squared Error (MSE) (we will explore evaluation metrics in more detail later on!).\n",
        "\n",
        "**7. Hyperparameter Tuning:**\n",
        "\n",
        "Fine-tune the model's hyperparameters to optimize its performance. Hyperparameters are settings that are not learned during training but affect the learning process, such as learning rate, regularization strength, and number of layers. Techniques like grid search or random search can help identify optimal hyperparameters. Each machine learning model typically has its own set of hyperparameters. Hyperparameters are parameters that are not learned during the training process but are set before training and affect how the model learns from the data. They control various aspects of the learning algorithm and model architecture. When you start working on your machine learning model, you will learn more about that model's hyperparameters!\n",
        "\n",
        "**8. Model Validation:**\n",
        "\n",
        "Once the hyperparameters are tuned, validate the model's performance on a separate validation set. This step is crucial to ensure that the hyperparameter tuning process didn't lead to overfitting on the test set.\n",
        "\n",
        "**9. Monitoring and Maintenance:**\n",
        "\n",
        "Continuously monitor the model's performance in the real world. As new data becomes available, periodically retrain the model using updated data to ensure it remains accurate and relevant."
      ],
      "metadata": {
        "id": "5hiiRVM96LvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:**\n",
        "\n",
        " Please edit this cell to include two new things you learned about training a supervised machine learning model:"
      ],
      "metadata": {
        "id": "_a9RBbgY9RTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training/Testing the Supervised Model"
      ],
      "metadata": {
        "id": "9nZvFw_q59bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing the model is extremely important. Please read [this](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data) article on training and testing. It is common practice to have 80% of your data go to training the model, 10% go to testing your model, and the remaining 10% go to validating your model. But, you don't have to follow that split!\n"
      ],
      "metadata": {
        "id": "zZxhEduw6DoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:**\n",
        "\n",
        "Please edit this cell to include why we don't use all of our data to train the model:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2YUEjVQl-4C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the main reasons we don't include all of our data to train the model is because of the concept of over-fitting, which refers to creating a model that matches the training data so closely that the model fails to make correct predictions on new data.\n",
        "\n",
        "\n",
        "We essentially want the training set acts as a canvas for our models to learn from. We, therefore, use randomness to gather a diverse set of data samples, each representing a unique facet of the problem at hand. This random selection ensures that no single aspect dominates, allowing the model to grasp the broader landscape and make good predictions for unseen data. By maintaining a proportional representation of each sample, we prevent biases from distorting the model's understanding. This mosaic of diverse and balanced data equips our models to generalize effectively, adapting its knowledge to new scenarios with accuracy."
      ],
      "metadata": {
        "id": "4a-kpK-Q_Gkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting is a big reason for including a test set. A test set is crucial for evaluating how well a trained machine learning model performs on new, unseen data. It helps us assess if the model's learned patterns generalize effectively, prevents overfitting, enables unbiased evaluation, aids in model selection and tuning, provides an estimate of real-world performance, and builds trust in the model's capabilities beyond the training data."
      ],
      "metadata": {
        "id": "lHUo4uz6Ak1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1"
      ],
      "metadata": {
        "id": "auaFp5V4CYJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the last part of this notebook, you will choose a dataset from [Kaggle](https://www.kaggle.com/) and upload it into this notebook and split into a training and test set (we will not include a validation set for this exercise). The steps to complete this exercise include:\n",
        "1. Create an account on [Kaggle](https://https://www.kaggle.com/) (a platform that data scientists often use to get datasets for exploration and the platform we will use for our project)\n",
        "2. Choose a [dataset](https://https://www.kaggle.com/datasets) and download it (make sure it isn't too big!)\n",
        "3. Uploading the dataset to this google collab\n",
        "4. Splitting the dataset into a train/test set (you can choose to have 80% go to train and 20% go to test)\n",
        "\n",
        "There are instructions on how to upload a dataset to this notebook below. There is also a coding cell at the bottom of the notebook that you can use to write your code.\n",
        "\n"
      ],
      "metadata": {
        "id": "r1CPukRCAdF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Uploading a dataset to this notebook\n",
        "\n",
        "**1. Upload the Dataset to Google Drive:**\n",
        "\n",
        "Before uploading the dataset to Colab, it's recommended to first upload the dataset to your Google Drive. This allows for seamless integration between Google Drive and Google Colab.\n",
        "\n",
        "**2. Mount Google Drive in Colab:**\n",
        "\n",
        "In your Colab notebook, you need to mount your Google Drive to access the files you uploaded. Run the following code cell to mount Google Drive:"
      ],
      "metadata": {
        "id": "4_EoIYZi0-66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "I5EEJOqHzzMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Navigate to the Dataset:**\n",
        "\n",
        "After mounting Google Drive, you can navigate to the directory where you uploaded the dataset. Use the file path based on the path in your Google Drive. Essentially, know where you put your dataset in your Google Drive.\n",
        "\n",
        "**4. Load the Dataset:**\n",
        "\n",
        "Once you've navigated to the directory, you can load the dataset using Python code. For example, if you have a CSV file named \"data.csv\" in a folder named \"datasets,\" you can load it like this if the dataset is in the location: /content/drive/My Drive/datasets/data.csv\n",
        "\n",
        "\n",
        "dataset_path = '/content/drive/My Drive/datasets/data.csv'\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "**5. Start using the dataset stored in df!**"
      ],
      "metadata": {
        "id": "ecFdJoHz1JVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Another helpful tip:**\n",
        "\n",
        "To split your data randomly, look into the function sample!\n",
        "\n"
      ],
      "metadata": {
        "id": "whaGST9-EV4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here!\n",
        "import pandas as pd\n",
        "\n"
      ],
      "metadata": {
        "id": "ZzSeTvAREjlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have finished this notebook!! Well-Done!!"
      ],
      "metadata": {
        "id": "76vtbWRnEyNe"
      }
    }
  ]
}